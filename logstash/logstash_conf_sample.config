input{
  // csv file 형태로 받는다
  file {
    // 경로 추후 지정 필요  
    path => "...." 
    start_position => beginning // default 값은 "end" :: 실시간 log는 최신것 부터 읽기 때문에
    // sincedb_path => "/dev/null"
    // 최초로 logstash에 데이터를 입력한 이후에는 start_position을 무시하고 
    // sincedb_path에 저장된 부분부터 읽기 시작 
    // 개발이나 테스트 단계에서는 sincedb 정보를 저장하지 않도록 "/dev/null"으로 설정
    codec => plain { charset => "UTF-8"}
  }

}

filter{

  // 입력받은 csv파일을 아래와 같이 쓰겠다
  csv{

    // csv파일의 각 header를 아래 컬럼 데이터로 집어넣고
    column => ["전투 인덱스",
            "전투 승리", "전투 트로피", "전투 랭크",
            "쿠키 인덱스",
            "쿠키 종류 인덱스", "쿠키 별", "쿠키 레벨", "쿠키 스킬 레벨", "쿠키 전투력",
            "쿠키 가한 피해량", "쿠키 받은 피해량", "쿠키 준 회복량", "쿠키 받은 회복량",
            "보물 인덱스_1", "보물 레벨_1",
            "보물 인덱스_2", "보물 레벨_2",
            "보물 인덱스_3", "보물 레벨_3",]
    // comma seperated
    separator => ","
  }
=> "integer"

["전투_인덱스","전투_승리","전투_트로피","전투_랭크","쿠키_index","쿠키_name","쿠키_star","쿠키_level","쿠키_skillLevel","쿠키_가한 피해량","쿠키_받은 피해량","쿠키_준 회복량","쿠키_받은 회복량","보물_name1","보물_name2","보물_name3","보물_level1","보물_level2","보물_level3"]

//   # Grok 은 포기하고, 아래 sample만 남겨둠
//   # 포기한 이유는 1) real-time에 input을 받는 게 아니고
//   # 2) column 개수가 너무 많아 차라리 전처리가 나음
//   #grok {
//   # match => { "message" =>
//   #"%{TIMESTAMP_ISO8601:timestamp}
//   #%{LOGLEVEL:log-level}
//   #\[%{DATA:class}\]:%{GREEDYDATA:message}" }
//   #}
  
  mutate {
    convert => {
        "전투 인덱스" => "keyword",
        "전투 승리" => "text",
        "전투 트로피" => "integer",
        "전투 랭크" => "text",
        "쿠키 인덱스" => "integer",
        "쿠키 종류 인덱스" => "integer",
        "쿠키 별" => "integer",
        "쿠키 레벨" => "integer",
        "쿠키 스킬 레벨" => "integer",
        "쿠키 전투력" => "integer",
        "쿠키 가한 피해량" => "double",
        "쿠키 받은 피해량" => "double",
        "쿠키 준 회복량" => "double",
        "쿠키 받은 회복량" => "double",
        "보물 인덱스_1" => "integer",
        "보물 인덱스_2" => "integer",
        "보물 인덱스_3" => "integer",
        "보물 레벨_1" => "integer",
        "보물 레벨_2" => "integer",
        "보물 레벨_3" => "integer",
    }
  }


  mutate{
    # elasticsearch로 들어가면 몇 개의 필드는 기본 생성이 됨
    # 특히 message 필드는 데이터를 한방에 때려넣기 때문에 중복
    remove_field => ["message", "host", "path"]
  }

}

output{
// 필터링된 데이터는 엘라스틱으로 밀어넣는다 / 이부분도 알맞게 수정 필요
  elasticsearch{index =>'aa' hosts => 'xxx:9200'}
  stdout{ codec => rubydebug }
}